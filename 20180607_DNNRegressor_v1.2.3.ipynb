{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 更新版本v1.1到v1.2\n",
    "\n",
    "1. 特征标准化\n",
    "2. 尝试其他优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(r'G:\\pycharm-workspace\\2018ATEC\\data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Install\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-930b1c5ac39c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'{:.4f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0manti_fraud_dataframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"atec_anti_fraud_train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0manti_fraud_dataframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manti_fraud_dataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0manti_fraud_dataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m anti_fraud_dataframe = anti_fraud_dataframe.reindex(\n",
      "\u001b[1;32mD:\\Install\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Install\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 455\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    456\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Install\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1067\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1069\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1070\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'as_recarray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Install\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1837\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1839\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1840\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._concatenate_chunks\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "from IPython import display\n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.data import Dataset\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "pd.options.display.max_rows = 10\n",
    "pd.set_option('display.max_columns', 350)\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "anti_fraud_dataframe = pd.read_csv(\"atec_anti_fraud_train.csv\")\n",
    "anti_fraud_dataframe = anti_fraud_dataframe[anti_fraud_dataframe.label != -1]\n",
    "anti_fraud_dataframe = anti_fraud_dataframe.reindex(\n",
    "    np.random.permutation(anti_fraud_dataframe.index))\n",
    "anti_fraud_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(anti_fraud_dataframe):\n",
    "    selected_feature = anti_fraud_dataframe[\n",
    "        [\"f28\", \n",
    "         \"f29\", \n",
    "         \"f30\", \n",
    "         \"f31\", \n",
    "         \"f52\", \n",
    "         \"f53\", \n",
    "         \"f111\", \n",
    "         \"f112\"\n",
    "#          \"f113\", \n",
    "#          \"f114\", \n",
    "#          \"f115\", \n",
    "#          \"f116\", \n",
    "#          \"f185\", \n",
    "#          \"f259\", \n",
    "#          \"f260\", \n",
    "#          \"f261\", \n",
    "#          \"f270\", \n",
    "#          \"f271\"\n",
    "        ]]\n",
    "    processed_features = selected_feature.copy()\n",
    "    processed_features = processed_features.fillna(-1)\n",
    "    return processed_features\n",
    "\n",
    "def preprocess_targets(anti_fraud_dataframe):\n",
    "    output_targets = pd.DataFrame()\n",
    "    output_targets[\"label\"] = anti_fraud_dataframe[\"label\"]\n",
    "    return output_targets\n",
    "\n",
    "display.display(preprocess_features(anti_fraud_dataframe))\n",
    "display.display(preprocess_targets(anti_fraud_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_feature_columns(input_features):\n",
    "    return set([tf.feature_column.numeric_column(my_feature)\n",
    "               for my_feature in input_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n",
    "    features = {key:np.array(value) for key,value in dict(features).items()}\n",
    "    \n",
    "    ds = Dataset.from_tensor_slices((features,targets))\n",
    "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(10000)\n",
    "    \n",
    "    features, labels = ds.make_one_shot_iterator().get_next()\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_regression_model(\n",
    "    my_optimizer,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    hidden_units,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets):\n",
    "    \n",
    "    periods = 10\n",
    "    steps_per_period = steps / periods\n",
    "    \n",
    "    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "    dnn_regressor = tf.estimator.DNNRegressor(\n",
    "        feature_columns=construct_feature_columns(training_examples),\n",
    "        hidden_units=hidden_units,\n",
    "        optimizer=my_optimizer\n",
    "    )\n",
    "    \n",
    "    training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                                            training_targets[\"label\"], \n",
    "                                            batch_size=batch_size)\n",
    "    predict_training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                                                    training_targets[\"label\"], \n",
    "                                                    num_epochs=1, \n",
    "                                                    shuffle=False)\n",
    "    predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n",
    "                                                      validation_targets[\"label\"], \n",
    "                                                      num_epochs=1, \n",
    "                                                      shuffle=False)\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    print(\"RMSE (on training data):\")\n",
    "    training_rmse = []\n",
    "    validation_rmse = []\n",
    "    for period in range (0, periods):\n",
    "        dnn_regressor.train(\n",
    "            input_fn=training_input_fn,\n",
    "            steps=steps_per_period\n",
    "        )\n",
    "        training_predictions = dnn_regressor.predict(input_fn=predict_training_input_fn)\n",
    "        training_predictions = np.array([item['predictions'][0] for item in training_predictions])\n",
    "\n",
    "        validation_predictions = dnn_regressor.predict(input_fn=predict_validation_input_fn)\n",
    "        validation_predictions = np.array([item['predictions'][0] for item in validation_predictions])\n",
    "\n",
    "        training_root_mean_squared_error = math.sqrt(\n",
    "            metrics.mean_squared_error(training_predictions, training_targets))\n",
    "        validation_root_mean_squared_error = math.sqrt(\n",
    "            metrics.mean_squared_error(validation_predictions, validation_targets))\n",
    "        print(\"  period %02d : %0.2f\" % (period, training_root_mean_squared_error))\n",
    "        training_rmse.append(training_root_mean_squared_error)\n",
    "        validation_rmse.append(validation_root_mean_squared_error)\n",
    "    print(\"Model training finished.\")\n",
    "    \n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.xlabel(\"Periods\")\n",
    "    plt.title(\"Root Mean Squared Error vs. Periods\")\n",
    "    plt.tight_layout()\n",
    "    plt.plot(training_rmse, label=\"training\")\n",
    "    plt.plot(validation_rmse, label=\"validation\")\n",
    "    plt.legend()\n",
    "\n",
    "    print(\"Final RMSE (on training data):   %0.2f\" % training_root_mean_squared_error)\n",
    "    print(\"Final RMSE (on validation data): %0.2f\" % validation_root_mean_squared_error)\n",
    "\n",
    "    return dnn_regressor, training_rmse, validation_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_scale(series):\n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    scale = (max_val - min_val) / 2.0\n",
    "    return series.apply(lambda x:((x - min_val) / scale) - 1.0)\n",
    "\n",
    "def log_normalize(series):\n",
    "    return series.apply(lambda x:math.log(x+1.0))\n",
    "\n",
    "def clip(series, clip_to_min, clip_to_max):\n",
    "    return series.apply(lambda x:(\n",
    "        min(max(x, clip_to_min), clip_to_max)))\n",
    "\n",
    "def z_score_normalize(series):\n",
    "    mean = series.mean()\n",
    "    std_dv = series.std()\n",
    "    return series.apply(lambda x:(x - mean) / std_dv)\n",
    "\n",
    "def binary_threshold(series, threshold):\n",
    "    return series.apply(lambda x:(1 if x > threshold else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(examples_dataframe):\n",
    "    processed_features = examples_dataframe.copy()\n",
    "    \n",
    "    processed_features[\"f111\"] = linear_scale(examples_dataframe[\"f111\"])\n",
    "    processed_features[\"f112\"] = linear_scale(examples_dataframe[\"f112\"])\n",
    "    \n",
    "    return processed_features\n",
    "\n",
    "normalized_dataframe = normalize(preprocess_features(anti_fraud_dataframe))\n",
    "normalized_training_examples = normalized_dataframe.head(696312)\n",
    "normalized_validation_examples = normalized_dataframe.tail(298419)\n",
    "\n",
    "training_targets = preprocess_targets(anti_fraud_dataframe.head(696312))\n",
    "validation_targets = preprocess_targets(anti_fraud_dataframe.tail(298419))\n",
    "\n",
    "print(\"Training examples summary:\")\n",
    "display.display(normalized_training_examples.describe())\n",
    "print(\"Validation examples summary:\")\n",
    "display.display(normalized_validation_examples.describe())\n",
    "\n",
    "print(\"Training targets summary:\")\n",
    "display.display(training_targets.describe())\n",
    "print(\"Validation targets summary:\")\n",
    "display.display(validation_targets.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_regressor, adagrad_training_losses, adagrad_validation_losses = train_nn_regression_model(\n",
    "    my_optimizer=tf.train.AdagradOptimizer(learning_rate=0.5),\n",
    "    steps=500,\n",
    "    batch_size=100,\n",
    "    hidden_units=[10, 10],\n",
    "    training_examples=normalized_training_examples,\n",
    "    training_targets=training_targets,\n",
    "    validation_examples=normalized_validation_examples,\n",
    "    validation_targets=validation_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_regressor1, adam_training_losses, adam_validation_losses = train_nn_regression_model(\n",
    "    my_optimizer=tf.train.AdamOptimizer(learning_rate=0.009),\n",
    "    steps=500,\n",
    "    batch_size=100,\n",
    "    hidden_units=[10, 10],\n",
    "    training_examples=normalized_training_examples,\n",
    "    training_targets=training_targets,\n",
    "    validation_examples=normalized_validation_examples,\n",
    "    validation_targets=validation_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_regressor2, gradientdescent_training_losses, gradientdescent_validation_losses = train_nn_regression_model(\n",
    "    my_optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.005),\n",
    "    steps=5000,\n",
    "    batch_size=50,\n",
    "    hidden_units=[10, 10],\n",
    "    training_examples=normalized_training_examples,\n",
    "    training_targets=training_targets,\n",
    "    validation_examples=normalized_validation_examples,\n",
    "    validation_targets=validation_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel(\"RMSE\")\n",
    "plt.xlabel(\"Periods\")\n",
    "plt.title(\"Root Mean Squared Error vs. Periods\")\n",
    "plt.plot(adagrad_validation_losses, label='Adagrad validation')\n",
    "plt.plot(adam_validation_losses, label='Adam validation')\n",
    "plt.plot(gradientdescent_validation_losses, label='GradientDescent validation')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_fraud_dataframe_test_data = pd.read_csv(\"atec_anti_fraud_test_a.csv\")\n",
    "display.display(anti_fraud_dataframe_test_data.describe())\n",
    "\n",
    "anti_fraud_dataframe_test_data['label'] = 0.0\n",
    "test_examples = normalize(preprocess_features(anti_fraud_dataframe_test_data))\n",
    "print(\"Test examples summary:\")\n",
    "display.display(test_examples.describe())\n",
    "\n",
    "test_targets = pd.DataFrame()\n",
    "test_targets[\"label\"] = (anti_fraud_dataframe_test_data[\"label\"])\n",
    "display.display(test_targets.describe())\n",
    "\n",
    "predict_testing_input_fn = lambda: my_input_fn(test_examples, \n",
    "                                               test_targets[\"label\"], \n",
    "                                               num_epochs=1, \n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = dnn_regressor.predict(input_fn=predict_testing_input_fn)\n",
    "test_predictions = np.array([item['predictions'][0] for item in test_predictions])\n",
    "display.display(test_predictions, len(test_predictions))\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "predictions['score'] = pd.Series(test_predictions)\n",
    "predictions['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([anti_fraud_dataframe_test_data[\"id\"], predictions['score']], axis=1)\n",
    "result.describe()\n",
    "\n",
    "result.loc[result[\"score\"]<0, \"score\"] = 0\n",
    "result.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[[\"id\", \"score\"]].to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
